{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data_1.txt\", \"r\") as file:\n",
    "    text = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Expression Tokenization:\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', 'In', 'basic', 'classification', 'tasks', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', 'For', 'example', 'in', 'multiclass', 'classification', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', 'in', 'open', 'class', 'classification', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', 'and', 'in', 'sequence', 'classification', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified']\n"
     ]
    }
   ],
   "source": [
    "regex_tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(\"Regular Expression Tokenization:\")\n",
    "print(regex_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK Tokenization:\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', '.', 'In', 'basic', 'classification', 'tasks', ',', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', ',', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', '.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', '.', 'For', 'example', ',', 'in', 'multiclass', 'classification', ',', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', ';', 'in', 'open-class', 'classification', ',', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', ';', 'and', 'in', 'sequence', 'classification', ',', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk_tokens = word_tokenize(text)\n",
    "print(\"\\nNLTK Tokenization:\")\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TextBlob Tokenization:\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', 'In', 'basic', 'classification', 'tasks', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', 'For', 'example', 'in', 'multiclass', 'classification', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', 'in', 'open-class', 'classification', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', 'and', 'in', 'sequence', 'classification', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified']\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(text)\n",
    "textblob_tokens = blob.words  # Note: blob.words returns a WordList\n",
    "print(\"\\nTextBlob Tokenization:\")\n",
    "print(list(textblob_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy Tokenization:\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', '.', 'In', 'basic', '\\n', 'classification', 'tasks', ',', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', ',', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', '.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', '.', 'For', 'example', ',', 'in', 'multiclass', 'classification', ',', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', ';', 'in', 'open', '-', 'class', 'classification', ',', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', ';', 'and', 'in', 'sequence', 'classification', ',', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(\"\\nspaCy Tokenization:\")\n",
    "print(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after removing stop words and punctuation:\n",
      "['Classification', 'task', 'choosing', 'correct', 'class', 'label', 'given', 'input', 'basic', 'classification', 'tasks', 'input', 'considered', 'isolation', 'inputs', 'set', 'labels', 'defined', 'advance', 'basic', 'classification', 'task', 'number', 'interesting', 'variants', 'example', 'multiclass', 'classification', 'instance', 'may', 'assigned', 'multiple', 'labels', 'open-class', 'classification', 'set', 'labels', 'defined', 'advance', 'sequence', 'classification', 'list', 'inputs', 'jointly', 'classified']\n",
      "\n",
      "Stop words found in the text corpus:\n",
      "['is', 'the', 'of', 'the', 'for', 'a', 'In', 'each', 'is', 'in', 'from', 'all', 'other', 'and', 'the', 'of', 'is', 'in', 'The', 'has', 'a', 'of', 'For', 'in', 'each', 'be', 'in', 'the', 'of', 'is', 'not', 'in', 'and', 'in', 'a', 'of', 'are']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Use the tokens obtained via NLTK tokenization (nltk_tokens) for this demonstration\n",
    "# ------------------------------\n",
    "# 3.1. Remove stop words and punctuation\n",
    "filtered_tokens = [\n",
    "    token for token in nltk_tokens\n",
    "    if token.lower() not in stop_words and token not in string.punctuation\n",
    "]\n",
    "\n",
    "print(\"\\nTokens after removing stop words and punctuation:\")\n",
    "print(filtered_tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 3.2. Identify and print the stop words found in the text corpus\n",
    "# ------------------------------\n",
    "stop_words_in_text = [\n",
    "    token for token in nltk_tokens if token.lower() in stop_words\n",
    "]\n",
    "print(\"\\nStop words found in the text corpus:\")\n",
    "print(stop_words_in_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification is the task of choosing the correct class label for a given input. In basic\\nclassification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance. The basic classification task has a number of interesting variants. For example, in multiclass classification, each instance may be assigned multiple labels; in open-class classification, the set of labels is not defined in advance; and in sequence classification, a list of inputs are jointly classified.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Data_1.txt\", \"r\") as file:\n",
    "    text = file.read().strip()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text ~~~~~~~~~~\n",
      " ['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', 'In', 'basic', 'classification', 'tasks', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', 'For', 'example', 'in', 'multiclass', 'classification', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', 'in', 'open-class', 'classification', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', 'and', 'in', 'sequence', 'classification', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified']\n",
      "\n",
      "Stemming using Regular Expression ~~~~~~~~~~\n",
      " ['Classification', 'i', 'the', 'task', 'of', 'choos', 'the', 'correct', 'clas', 'label', 'for', 'a', 'given', 'input', 'In', 'basic', 'classification', 'task', 'each', 'input', 'i', 'consider', 'in', 'isolation', 'from', 'all', 'other', 'input', 'and', 'the', 'set', 'of', 'label', 'i', 'defin', 'in', 'advance', 'The', 'basic', 'classification', 'task', 'ha', 'a', 'number', 'of', 'interest', 'variant', 'For', 'example', 'in', 'multiclas', 'classification', 'each', 'instance', 'may', 'be', 'assign', 'multiple', 'label', 'in', 'open-clas', 'classification', 'the', 'set', 'of', 'label', 'i', 'not', 'defin', 'in', 'advance', 'and', 'in', 'sequence', 'classification', 'a', 'list', 'of', 'input', 'are', 'jointly', 'classifi']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "Re_tokens = re.findall(r'\\b[\\w-]+\\b', text)\n",
    "regex_stemmed = [re.sub(r'(ing|ed|s|able)$', '', word) for word in Re_tokens ]\n",
    "\n",
    "print(\"Original Text ~~~~~~~~~~\\n\",Re_tokens)\n",
    "print(\"\\nStemming using Regular Expression ~~~~~~~~~~\\n\",regex_stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming using Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text ~~~~~~~~~~\n",
      " ['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', '.', 'In', 'basic', 'classification', 'tasks', ',', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', ',', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', '.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', '.', 'For', 'example', ',', 'in', 'multiclass', 'classification', ',', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', ';', 'in', 'open-class', 'classification', ',', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', ';', 'and', 'in', 'sequence', 'classification', ',', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified', '.']\n",
      "\n",
      "Stemming using Porter Stemmer ~~~~~~~~~~\n",
      " ['classif', 'is', 'the', 'task', 'of', 'choos', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', '.', 'in', 'basic', 'classif', 'task', ',', 'each', 'input', 'is', 'consid', 'in', 'isol', 'from', 'all', 'other', 'input', ',', 'and', 'the', 'set', 'of', 'label', 'is', 'defin', 'in', 'advanc', '.', 'the', 'basic', 'classif', 'task', 'ha', 'a', 'number', 'of', 'interest', 'variant', '.', 'for', 'exampl', ',', 'in', 'multiclass', 'classif', ',', 'each', 'instanc', 'may', 'be', 'assign', 'multipl', 'label', ';', 'in', 'open-class', 'classif', ',', 'the', 'set', 'of', 'label', 'is', 'not', 'defin', 'in', 'advanc', ';', 'and', 'in', 'sequenc', 'classif', ',', 'a', 'list', 'of', 'input', 'are', 'jointli', 'classifi', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "PS_tokens = word_tokenize(text)  \n",
    "\n",
    "print(\"Original Text ~~~~~~~~~~\\n\",PS_tokens)\n",
    "print(\"\\nStemming using Porter Stemmer ~~~~~~~~~~\\n\",[ps.stem(w) for w in PS_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming using Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text ~~~~~~~~~~\n",
      " ['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', '.', 'In', 'basic', 'classification', 'tasks', ',', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', ',', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', '.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', '.', 'For', 'example', ',', 'in', 'multiclass', 'classification', ',', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', ';', 'in', 'open-class', 'classification', ',', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', ';', 'and', 'in', 'sequence', 'classification', ',', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified', '.']\n",
      "\n",
      "Stemming using Lancaster Stemmer ~~~~~~~~~~\n",
      " ['class', 'is', 'the', 'task', 'of', 'choos', 'the', 'correct', 'class', 'label', 'for', 'a', 'giv', 'input', '.', 'in', 'bas', 'class', 'task', ',', 'each', 'input', 'is', 'consid', 'in', 'isol', 'from', 'al', 'oth', 'input', ',', 'and', 'the', 'set', 'of', 'label', 'is', 'defin', 'in', 'adv', '.', 'the', 'bas', 'class', 'task', 'has', 'a', 'numb', 'of', 'interest', 'vary', '.', 'for', 'exampl', ',', 'in', 'multiclass', 'class', ',', 'each', 'inst', 'may', 'be', 'assign', 'multipl', 'label', ';', 'in', 'open-class', 'class', ',', 'the', 'set', 'of', 'label', 'is', 'not', 'defin', 'in', 'adv', ';', 'and', 'in', 'sequ', 'class', ',', 'a', 'list', 'of', 'input', 'ar', 'joint', 'class', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "LS_tokens = word_tokenize(text)\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "print(\"Original Text ~~~~~~~~~~\\n\",LS_tokens)\n",
    "print(\"\\nStemming using Lancaster Stemmer ~~~~~~~~~~\\n\", [lancaster.stem(t) for t in LS_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\User/nltk_data', 'c:\\\\Users\\\\User\\\\Documents\\\\Coding\\\\Text\\\\.venv\\\\nltk_data', 'c:\\\\Users\\\\User\\\\Documents\\\\Coding\\\\Text\\\\.venv\\\\share\\\\nltk_data', 'c:\\\\Users\\\\User\\\\Documents\\\\Coding\\\\Text\\\\.venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n",
    "from nltk import pos_tag, word_tokenize, RegexpTagger, CFG\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The big black dog barked at the white cat and chased away.\n"
     ]
    }
   ],
   "source": [
    "with open(\"Data_2.txt\", \"r\") as file:\n",
    "    sentence = file.read().strip()\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK POS Tagger output:\n",
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "nltk_tags = pos_tag(tokens)\n",
    "print(\"NLTK POS Tagger output:\")\n",
    "print(nltk_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Textblob POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TextBlob POS Tagger output:\n",
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(sentence)\n",
    "textblob_tags = blob.tags\n",
    "print(\"\\nTextBlob POS Tagger output:\")\n",
    "print(textblob_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),              # gerunds\n",
    "    (r'.*ed$', 'VBD'),               # simple past\n",
    "    (r'.*es$', 'VBZ'),               # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),              # modals\n",
    "    (r'.*\\'s$', 'NN$'),              # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n",
    "    (r'.*ly$', 'RB'),                 # adverbs   \n",
    "    (r'The', 'DT'),                  # determiner 'The'  (more specific)\n",
    "    (r'.*', 'NN')                    # nouns (default)  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regular Expression Tagger output:\n",
      "[('The', 'DT'), ('big', 'NN'), ('black', 'NN'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'NN'), ('the', 'NN'), ('white', 'NN'), ('cat', 'NN'), ('and', 'NN'), ('chased', 'VBD'), ('away', 'NN'), ('.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "regexp_tagger = RegexpTagger(patterns)\n",
    "regex_tags = regexp_tagger.tag(tokens)\n",
    "print(\"\\nRegular Expression Tagger output:\")\n",
    "print(regex_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawing Parse Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = CFG.fromstring(\"\"\"\n",
    "S    -> NP VP\n",
    "NP   -> Det AdjP N\n",
    "AdjP -> Adj | Adj AdjP\n",
    "VP   -> V PP | V Adv | VP Conj VP\n",
    "PP   -> P NP\n",
    "Det  -> 'The' | 'the'\n",
    "N    -> 'dog' | 'cat'\n",
    "Adj  -> 'big' | 'black' | 'white'\n",
    "V    -> 'barked' | 'chased'\n",
    "Adv -> 'away'\n",
    "P    -> 'at'\n",
    "Conj -> 'and'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parse Trees:\n",
      "(S\n",
      "  (NP (Det The) (AdjP (Adj big) (AdjP (Adj black))) (N dog))\n",
      "  (VP\n",
      "    (VP\n",
      "      (V barked)\n",
      "      (PP (P at) (NP (Det the) (AdjP (Adj white)) (N cat))))\n",
      "    (Conj and)\n",
      "    (VP (V chased) (Adv away))))\n",
      "                               S                                        \n",
      "      _________________________|____________                             \n",
      "     |                                      VP                          \n",
      "     |                              ________|____________________        \n",
      "     |                             VP                |           |      \n",
      "     |                    _________|___              |           |       \n",
      "     NP                  |             PP            |           |      \n",
      "  ___|______________     |      _______|____         |           |       \n",
      " |      AdjP        |    |     |            NP       |           |      \n",
      " |    ___|_____     |    |     |    ________|____    |           |       \n",
      " |   |        AdjP  |    |     |   |       AdjP  |   |           VP     \n",
      " |   |         |    |    |     |   |        |    |   |      _____|___    \n",
      "Det Adj       Adj   N    V     P  Det      Adj   N  Conj   V        Adv \n",
      " |   |         |    |    |     |   |        |    |   |     |         |   \n",
      "The big      black dog barked  at the     white cat and  chased     away\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = ['The', 'big', 'black', 'dog', 'barked', 'at', 'the', 'white', 'cat', 'and', 'chased', 'away']\n",
    "parser = nltk.ChartParser(grammar)\n",
    "print(\"\\nParse Trees:\")\n",
    "for tree in parser.parse(tokens):\n",
    "    print(tree)\n",
    "    tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = [\n",
    "    \"<s> He read a book </s>\".split(),\n",
    "    \"<s> I read a different book </s>\".split(),\n",
    "    \"<s> He read a book by Danielle </s>\".split()\n",
    "]\n",
    "target_sentence = \"<s> I read a book by Danielle </s>\".split()\n",
    "\n",
    "\n",
    "bigram_counts = {}   # holds values\n",
    "unigram_counts = {}  # holds thr preceding values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsmoothed Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each sentence in the training corpus to update counts\n",
    "for sentence in training_corpus:\n",
    "    for i in range(len(sentence) - 1):\n",
    "        prev_word = sentence[i]\n",
    "        next_word = sentence[i + 1]\n",
    "        if prev_word not in bigram_counts:\n",
    "            bigram_counts[prev_word] = {}\n",
    "        bigram_counts[prev_word][next_word] = bigram_counts[prev_word].get(next_word, 0) + 1\n",
    "        unigram_counts[prev_word] = unigram_counts.get(prev_word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsmoothed bigram probabilities:\n",
      "P(I|<s>) = 1/3 = 0.3333333333333333\n",
      "P(read|I) = 1/1 = 1.0\n",
      "P(a|read) = 3/3 = 1.0\n",
      "P(book|a) = 2/3 = 0.6666666666666666\n",
      "P(by|book) = 1/3 = 0.3333333333333333\n",
      "P(Danielle|by) = 1/1 = 1.0\n",
      "P(</s>|Danielle) = 1/1 = 1.0\n",
      "Total unsmoothed probability: 0.07407407407407407\n"
     ]
    }
   ],
   "source": [
    "# Calculate unsmoothed bigram probability for the target sentence\n",
    "unsmoothed_prob = 1.0\n",
    "print(\"Unsmoothed bigram probabilities:\")\n",
    "for i in range(len(target_sentence) - 1):\n",
    "    prev_word = target_sentence[i]\n",
    "    next_word = target_sentence[i + 1]\n",
    "    count_bigram = 0\n",
    "    if prev_word in bigram_counts:\n",
    "        count_bigram = bigram_counts[prev_word].get(next_word, 0)\n",
    "    count_prev = unigram_counts.get(prev_word, 0)\n",
    "    prob = count_bigram / count_prev if count_prev != 0 else 0\n",
    "    unsmoothed_prob *= prob\n",
    "    print(\"P({}|{}) = {}/{} = {}\".format(next_word, prev_word, count_bigram, count_prev, prob))\n",
    "print(\"Total unsmoothed probability:\", unsmoothed_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothed Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Add-One Smoothed bigram probabilities:\n",
      "P(I|<s>) = (1+1)/(3+9) = 0.16666666666666666\n",
      "P(read|I) = (1+1)/(1+9) = 0.2\n",
      "P(a|read) = (3+1)/(3+9) = 0.3333333333333333\n",
      "P(book|a) = (2+1)/(3+9) = 0.25\n",
      "P(by|book) = (1+1)/(3+9) = 0.16666666666666666\n",
      "P(Danielle|by) = (1+1)/(1+9) = 0.2\n",
      "P(</s>|Danielle) = (1+1)/(1+9) = 0.2\n",
      "\n",
      "Total smoothed probability: 1.8518518518518515e-05\n"
     ]
    }
   ],
   "source": [
    "# Calculate smoothed bigram probabilities\n",
    "smoothed_prob = 1.0\n",
    "vocab_size = 9\n",
    "print(\"\\nAdd-One Smoothed bigram probabilities:\")\n",
    "for i in range(len(target_sentence) - 1):\n",
    "    prev_word = target_sentence[i]\n",
    "    next_word = target_sentence[i + 1]\n",
    "    count_bigram = 0\n",
    "    if prev_word in bigram_counts:\n",
    "        count_bigram = bigram_counts[prev_word].get(next_word, 0)\n",
    "    count_prev = unigram_counts.get(prev_word, 0)\n",
    "    prob = (count_bigram + 1) / (count_prev + vocab_size)\n",
    "    smoothed_prob *= prob\n",
    "    print(\"P({}|{}) = ({count_bigram}+1)/({count_prev}+{vocab_size}) = {prob}\".format(\n",
    "        next_word, prev_word, count_bigram=count_bigram, count_prev=count_prev, vocab_size=vocab_size, prob=prob))\n",
    "print(\"\\nTotal smoothed probability:\", smoothed_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
